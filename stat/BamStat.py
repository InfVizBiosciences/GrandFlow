#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
desc
"""

# ---------
# Change Logs:
#
# ---------
""""

This module provides functions to extract useful metrics
from Oxford Nanopore sequencing reads and alignments.


Data can be presented in the following formats, using the following functions:
- A sorted bam file
  process_bam(bamfile, threads)
- A standard fastq file
  process_fastq_plain(fastqfile, 'threads')
- A fastq file with metadata from MinKNOW or Albacore
  process_fastq_rich(fastqfile)
- A sequencing_summary file generated by Albacore
  process_summary(sequencing_summary.txt, 'readtype')

Fastq files can be compressed using gzip, bzip2 or bgzip.
The data is returned as a pandas DataFrame with standardized headernames for convenient extraction.
The functions perform logging while being called and extracting data.
"""

__author__ = 'Li Pidong'
__email__ = 'lipidong@126.com'
__version__ = '0.0.1'
__status__ = 'Dev'

import concurrent.futures as cfutures
import logging
import re
import sys
from functools import partial
import os
from os import path as opath
from argparse import ArgumentParser
from math import log

import numpy as np
import pandas as pd
from scipy import stats

import grandflow.plot.lib as grandplot
import pysam


def get_input(source,
              files,
              threads=4,
              readtype="1D",
              combine="simple",
              names=None,
              barcoded=False):
    """Get input and process accordingly.

    Data can be:
    - a uncompressed, bgzip, bzip2 or gzip compressed fastq file
    - a uncompressed, bgzip, bzip2 or gzip compressed fasta file
    - a rich fastq containing additional key=value information in the description,
      as produced by MinKNOW and albacore with the same compression options as above
    - a sorted bam file
    - a sorted cram file
    - a (compressed) sequencing_summary.txt file generated by albacore

    Handle is passed to the proper functions to get DataFrame with metrics
    Multiple files of the same type can be used to extract info from, which is done in parallel
    Arguments:
    - source: defines the input data type and the function that needs to be called
    - files: is a list of one or more files to operate on, from the type of <source>
    - threads: is the amount of workers which can be used
    - readtype: (only relevant for summary input) and specifies which columns have to be extracted
    - combine: is either 'simple' or 'track', with the difference that  with 'track' an additional
      field is created with the name of the dataset
    - names: if combine="track", the names to be used for the datasets. Needs to have same length as
      files, or None
    """
    proc_functions = {
        # 'fastq': process_fastq_plain,
        # 'fasta': process_fasta,
        'bam': process_bam,
        # 'summary': process_summary,
        # 'fastq_rich': process_fastq_rich,
        # 'fastq_minimal': process_fastq_minimal,
        # 'cram': process_cram
    }
    filethreads = min(len(files), threads)
    threadsleft = threads - filethreads
    with cfutures.ProcessPoolExecutor(max_workers=filethreads) as executor:
        extration_function = partial(
            proc_functions[source],
            threads=threadsleft,
            readtype=readtype,
            barcoded=barcoded)
        datadf = combine_dfs(
            dfs=[out for out in executor.map(extration_function, files)],
            names=names or files,
            method=combine)
    # datadf = calculate_start_time(datadf)
    logging.info("Nanoget: Gathered all metrics of {} reads".format(
        len(datadf)))
    if len(datadf) == 0:
        logging.critical("Nanoget: no reads retrieved.".format(len(datadf)))
        sys.exit("Fatal: No reads found in input.")
    else:
        return datadf


def ave_qual(quals):
    """Calculate average basecall quality of a read.

    Receive the integer quality scores of a read and return the average quality for that read
    First convert Phred scores to probabilities,
    calculate average error probability
    convert average back to Phred scale

    Return None for ZeroDivisionError
    """
    if quals:
        return -10 * log(sum([10**(q / -10) for q in quals]) / len(quals), 10)
    else:
        return None


def combine_dfs(dfs, names, method):
    """Combine dataframes.

    Combination is either done simple by just concatenating the DataFrames
    or performs tracking by adding the name of the dataset as a column."""
    if method == "track":
        res = list()
        for df, identifier in zip(dfs, names):
            df["dataset"] = identifier
            res.append(df)
        return pd.concat(res, ignore_index=True)
    elif method == "simple":
        return pd.concat(dfs, ignore_index=True)


def calculate_start_time(df):
    """Calculate the star_time per read.

    Time data is either
    a "time" (in seconds, derived from summary files) or
    a "timestamp" (in UTC, derived from fastq_rich format)
    and has to be converted appropriately in a datetime format time_arr

    For both the time_zero is the minimal value of the time_arr,
    which is then used to subtract from all other times

    In the case of method=track (and dataset is a column in the df) then this
    subtraction is done per dataset
    """
    if "time" in df:
        df["time_arr"] = pd.Series(df["time"], dtype='datetime64[s]')
    elif "timestamp" in df:
        df["time_arr"] = pd.Series(df["timestamp"], dtype="datetime64[ns]")
    else:
        return df
    if "dataset" in df:
        for dset in df["dataset"].unique():
            time_zero = df.loc[df["dataset"] == dset, "time_arr"].min()
            df.loc[df["dataset"] == dset, "start_time"] = \
                df.loc[df["dataset"] == dset, "time_arr"] - time_zero
    else:
        df["start_time"] = df["time_arr"] - df["time_arr"].min()
    return df.drop(["time", "timestamp", "time_arr"], axis=1, errors="ignore")


def check_existance(f):
    """Check if the file supplied as input exists."""
    if not opath.isfile(f):
        logging.error(
            "Nanoget: File provided doesn't exist or the path is incorrect: {}".
            format(f))
        sys.exit(
            "File provided doesn't exist or the path is incorrect: {}".format(
                f))

def check_bam(bam, samtype="bam"):
    """Check if bam file is valid.

    Bam file should:
    - exists
    - has an index (create if necessary)
    - is sorted by coordinate
    - has at least one mapped read
    """
    check_existance(bam)
    samfile = pysam.AlignmentFile(bam, "rb")
    if not samfile.has_index():
        pysam.index(bam)
        samfile = pysam.AlignmentFile(bam, "rb")  # Need to reload the samfile after creating index
        logging.info("Nanoget: No index for bam file could be found, created index.")
    if not samfile.header['HD']['SO'] == 'coordinate':
        logging.error("Nanoget: Bam file {} not sorted by coordinate!.".format(bam))
        sys.exit("Please use a bam file sorted by coordinate.")
    if samtype == "bam":
        logging.info("Nanoget: Bam file {} contains {} mapped and {} unmapped reads.".format(
            bam, samfile.mapped, samfile.unmapped))
        if samfile.mapped == 0:
            logging.error("Nanoget: Bam file {} does not contain aligned reads.".format(bam))
            sys.exit("FATAL: not a single read was mapped in bam file {}".format(bam))
    return samfile


def process_bam(bam, **kwargs):
    """Combines metrics from bam after extraction.

    Processing function: calls pool of worker functions
    to extract from a bam file the following metrics:
    -lengths
    -aligned lengths
    -qualities
    -aligned qualities
    -mapping qualities
    -edit distances to the reference genome scaled by read length
    Returned in a pandas DataFrame
    """
    logging.info(
        "Nanoget: Starting to collect statistics from bam file {}.".format(
            bam))
    samfile = check_bam(bam)
    chromosomes = samfile.references
    params = zip([bam] * len(chromosomes), chromosomes)
    with cfutures.ProcessPoolExecutor() as executor:
        datadf = pd.DataFrame(
            data=[res for sublist in executor.map(extract_from_bam, params) for res in sublist],
            columns=["readIDs", "quals", "aligned_quals", "lengths",
                     "aligned_lengths", "mapQ", "percentIdentity"]) \
            .dropna(axis='columns', how='all') \
            .dropna(axis='index', how='any')
    logging.info("Nanoget: bam {} contains {} primary alignments.".format(
        bam, datadf["lengths"].size))
    return datadf


def extract_from_bam(params):
    """Extracts metrics from bam.

    Worker function per chromosome
    loop over a bam file and create list with tuples containing metrics:
    -qualities
    -aligned qualities
    -lengths
    -aligned lengths
    -mapping qualities
    -edit distances to the reference genome scaled by read length
    """
    bam, chromosome = params
    samfile = pysam.AlignmentFile(bam, "rb")
    return [(read.query_name, ave_qual(read.query_qualities),
             ave_qual(read.query_alignment_qualities), read.query_length,
             read.query_alignment_length, read.mapping_quality, get_pID(read))
            for read in samfile.fetch(
                reference=chromosome, multiple_iterators=True)
            if not read.is_secondary]


def get_pID(read):
    """Return the percent identity of a read.

    based on the NM tag if present,
    if not calculate from MD tag and CIGAR string

    read.query_alignment_length can be zero in the case of ultra long reads aligned with minimap2 -L
    """
    try:
        return 100 * (1 - read.get_tag("NM") / read.query_alignment_length)
    except KeyError:
        try:
            return 100 * (
                1 -
                (parse_MD(read.get_tag("MD")) + parse_CIGAR(read.cigartuples)
                 ) / read.query_alignment_length)
        except KeyError:
            return None
    except ZeroDivisionError:
        return None


def parse_MD(MDlist):
    """Parse MD string to get number of mismatches and deletions."""
    return sum([len(item) for item in re.split('[0-9^]', MDlist)])


def parse_CIGAR(cigartuples):
    """Count the insertions in the read using the CIGAR string."""
    return sum([item[1] for item in cigartuples if item[0] == 1])


def filter_and_transform_data(datadf, settings):
    '''
    Perform filtering on the data based on arguments set on commandline
    - use aligned length or sequenced length (bam mode only)
    - drop outliers
    - drop reads longer than args.maxlength
    - use log10 scaled reads
    - downsample reads to args.downsample
    Return an accurate prefix which is added to plotnames using this filtered data
    '''
    length_prefix_list = list()
    settings["filtered"] = False
    if settings["alength"] and settings["bam"]:
        settings["lengths_pointer"] = "aligned_lengths"
        length_prefix_list.append("Aligned_")
        logging.info("Using aligned read lengths for plotting.")
    else:
        settings["lengths_pointer"] = "lengths"
        logging.info("Using sequenced read lengths for plotting.")
    if settings["drop_outliers"]:
        num_reads_prior = len(datadf)
        datadf = nanomath.remove_length_outliers(datadf, settings["lengths_pointer"])
        length_prefix_list.append("OutliersRemoved_")
        num_reads_post = len(datadf)
        logging.info("Removing {} length outliers for plotting.".format(
            str(num_reads_prior - num_reads_post)))
        settings["filtered"] = True
    if settings["maxlength"]:
        num_reads_prior = len(datadf)
        datadf = datadf.loc[datadf[settings["lengths_pointer"]] < settings["maxlength"]].copy()
        length_prefix_list.append("MaxLength-" + str(settings["maxlength"]) + '_')
        num_reads_post = len(datadf)
        logging.info("Removed {} reads longer than {}bp.".format(
            str(num_reads_prior - num_reads_post),
            str(settings["maxlength"])))
        settings["filtered"] = True
    if settings["minlength"]:
        num_reads_prior = len(datadf)
        datadf = datadf.loc[datadf[settings["lengths_pointer"]] > settings["minlength"]].copy()
        length_prefix_list.append("MinLength-" + str(settings["minlength"]) + '_')
        num_reads_post = len(datadf)
        logging.info("Removed {} reads shorter than {}bp.".format(
            str(num_reads_prior - num_reads_post),
            str(settings["minlength"])))
        settings["filtered"] = True
    if settings["minqual"]:
        num_reads_prior = len(datadf)
        datadf = datadf.loc[datadf["quals"] > settings["minqual"]].copy()
        num_reads_post = len(datadf)
        logging.info("Removing {} reads with quality below Q{}.".format(
            str(num_reads_prior - num_reads_post),
            str(settings["minqual"])))
        settings["filtered"] = True
    if settings["loglength"]:
        datadf["log_" + settings["lengths_pointer"]] = np.log10(datadf[settings["lengths_pointer"]])
        settings["lengths_pointer"] = "log_" + settings["lengths_pointer"]
        length_prefix_list.append("Log_")
        logging.info("Using Log10 scaled read lengths.")
        settings["logBool"] = True
    else:
        settings["logBool"] = False
    if settings["downsample"]:
        new_size = min(settings["downsample"], len(datadf.index))
        length_prefix_list.append("Downsampled_")
        logging.info("Downsampling the dataset from {} to {} reads".format(
            len(datadf.index), new_size))
        datadf = datadf.sample(new_size)
        settings["filtered"] = True
    if settings["percentqual"]:
        datadf["quals"] = datadf["quals"].apply(nanomath.phred_to_percent)
        logging.info("Converting quality scores to theoretical percent identities.")
    logging.info("Processed the reads, optionally filtered. {} reads left".format(str(len(datadf))))
    settings["length_prefix"] = ''.join(length_prefix_list)
    return(datadf, settings)


def make_plots(datadf, settings):
    '''
    Call plotting functions from grandplot
    settings["lengths_pointer"] is a column in the DataFrame specifying which lengths to use
    '''
    color = grandplot.check_valid_color(settings["color"])
    plotdict = {
        type: settings["plots"].count(type)
        for type in ["kde", "hex", "dot", 'pauvre']
    }
    plots = []
    if settings["N50"]:
        n50 = nanomath.get_N50(np.sort(datadf["lengths"]))
    else:
        n50 = None
    # plots.extend(
        # grandplot.length_plots(
            # array=datadf["lengths"],
            # name="Read length",
            # path=settings["path"],
            # n50=n50,
            # color=color,
            # figformat=settings["format"],
            # title=settings["title"]))
    # logging.info("Created length plots")
    if "quals" in datadf:
        plots.extend(
            grandplot.scatter(
                x=datadf[settings["lengths_pointer"]],
                y=datadf["quals"],
                names=['Read lengths', 'Average read quality'],
                path=settings["path"] + settings["length_prefix"] +
                "LengthvsQualityScatterPlot",
                color=color,
                figformat=settings["format"],
                plots=plotdict,
                log=settings["logBool"],
                title=settings["title"]))
        logging.info("Created LengthvsQual plot")
    if "aligned_lengths" in datadf and "lengths" in datadf:
        plots.extend(
            grandplot.scatter(
                x=datadf["aligned_lengths"],
                y=datadf["lengths"],
                names=["Aligned read lengths", "Sequenced read length"],
                path=settings["path"] +
                "AlignedReadlengthvsSequencedReadLength",
                figformat=settings["format"],
                plots=plotdict,
                color=color,
                title=settings["title"]))
        logging.info("Created AlignedLength vs Length plot.")
    if "mapQ" in datadf and "quals" in datadf:
        plots.extend(
            grandplot.scatter(
                x=datadf["mapQ"],
                y=datadf["quals"],
                names=["Read mapping quality", "Average basecall quality"],
                path=settings["path"] + "MappingQualityvsAverageBaseQuality",
                color=color,
                figformat=settings["format"],
                plots=plotdict,
                title=settings["title"]))
        logging.info("Created MapQvsBaseQ plot.")
        plots.extend(
            grandplot.scatter(
                x=datadf[settings["lengths_pointer"]],
                y=datadf["mapQ"],
                names=["Read length", "Read mapping quality"],
                path=settings["path"] + settings["length_prefix"] +
                "MappingQualityvsReadLength",
                color=color,
                figformat=settings["format"],
                plots=plotdict,
                log=settings["logBool"],
                title=settings["title"]))
        logging.info("Created Mapping quality vs read length plot.")
    if "percentIdentity" in datadf:
        minPID = np.percentile(datadf["percentIdentity"], 1)
        if "aligned_quals" in datadf:
            plots.extend(
                grandplot.scatter(
                    x=datadf["percentIdentity"],
                    y=datadf["aligned_quals"],
                    names=["Percent identity", "Average Base Quality"],
                    path=settings["path"] +
                    "PercentIdentityvsAverageBaseQuality",
                    color=color,
                    figformat=settings["format"],
                    plots=plotdict,
                    stat=stats.pearsonr,
                    minvalx=minPID,
                    title=settings["title"]))
            logging.info("Created Percent ID vs Base quality plot.")
        plots.extend(
            grandplot.scatter(
                x=datadf[settings["lengths_pointer"]],
                y=datadf["percentIdentity"],
                names=["Aligned read length", "Percent identity"],
                path=settings["path"] + "PercentIdentityvsAlignedReadLength",
                color=color,
                figformat=settings["format"],
                plots=plotdict,
                stat=stats.pearsonr,
                log=settings["logBool"],
                minvaly=minPID,
                title=settings["title"]))
        logging.info("Created Percent ID vs Length plot")
    return plots


def get_args():
    # epilog = """EXAMPLES:
    # Nanoplot --summary sequencing_summary.txt --loglength -o summary-plots-log-transformed
    # NanoPlot -t 2 --fastq reads1.fastq.gz reads2.fastq.gz --maxlength 40000 --plots hex dot
    # NanoPlot --color yellow --bam alignment1.bam alignment2.bam alignment3.bam --downsample 10000
    # """
    parser = ArgumentParser(
        description="Creates various plots for long read sequencing data.".
        upper(),
        # epilog=epilog,
        # formatter_class=utils.custom_formatter,
        add_help=False)
    general = parser.add_argument_group(title='General options')
    general.add_argument(
        "-h", "--help", action="help", help="show the help and exit")
    general.add_argument(
        "-v",
        "--version",
        help="Print version and exit.",
        action="version",
        version='NanoPlot {}'.format(__version__))
    general.add_argument(
        "-t",
        "--threads",
        help="Set the allowed number of threads to be used by the script",
        default=4,
        type=int)
    general.add_argument(
        "--verbose",
        help="Write log messages also to terminal.",
        action="store_true")
    general.add_argument(
        "--store",
        help="Store the extracted data in a pickle file for future plotting.",
        action="store_true")
    general.add_argument(
        "--raw",
        help="Store the extracted data in tab separated file.",
        action="store_true")
    general.add_argument(
        "-o",
        "--outdir",
        help="Specify directory in which output has to be created.",
        default=".")
    general.add_argument(
        "-p",
        "--prefix",
        help="Specify an optional prefix to be used for the output files.",
        default="",
        type=str)
    filtering = parser.add_argument_group(
        title='Options for filtering or transforming input prior to plotting')
    filtering.add_argument(
        "--maxlength",
        help="Drop reads longer than length specified.",
        type=int,
        metavar='N')
    filtering.add_argument(
        "--minlength",
        help="Drop reads shorter than length specified.",
        type=int,
        metavar='N')
    filtering.add_argument(
        "--drop_outliers",
        help="Drop outlier reads with extreme long length.",
        action="store_true")
    filtering.add_argument(
        "--downsample",
        help="Reduce dataset to N reads by random sampling.",
        type=int,
        metavar='N')
    filtering.add_argument(
        "--loglength",
        help="Logarithmic scaling of lengths in plots.",
        action="store_true")
    filtering.add_argument(
        "--percentqual",
        help="Use qualities as theoretical percent identities.",
        action="store_true")
    filtering.add_argument(
        "--alength",
        help="Use aligned read lengths rather than sequenced length (bam mode)",
        action="store_true")
    filtering.add_argument(
        "--minqual",
        help="Drop reads with an average quality lower than specified.",
        type=int,
        metavar='N')
    filtering.add_argument(
        "--readtype",
        help="Which read type to extract information about from summary. \
                                 Options are 1D, 2D, 1D2",
        default="1D",
        choices=['1D', '2D', '1D2'])
    filtering.add_argument(
        "--barcoded",
        help="Use if you want to split the summary file by barcode",
        action="store_true")
    visual = parser.add_argument_group(
        title='Options for customizing the plots created')
    visual.add_argument(
        "-c",
        "--color",
        help="Specify a color for the plots, must be a valid matplotlib color",
        default="#4CB391")
    visual.add_argument(
        "-f",
        "--format",
        help="Specify the output format of the plots.",
        default="png",
        type=str,
        choices=[
            'eps', 'jpeg', 'jpg', 'pdf', 'pgf', 'png', 'ps', 'raw', 'rgba',
            'svg', 'svgz', 'tif', 'tiff'
        ])
    visual.add_argument(
        "--plots",
        help="Specify which bivariate plots have to be made.",
        default=['kde', 'dot'],
        type=str,
        nargs='*',
        choices=['kde', 'hex', 'dot', 'pauvre'])
    visual.add_argument(
        "--listcolors",
        help="List the colors which are available for plotting and exit.",
        # action=utils.Action_Print_Colors,
        default=False)
    visual.add_argument(
        "--no-N50",
        help="Hide the N50 mark in the read length histogram",
        action="store_true")
    visual.add_argument(
        "--N50",
        help="Show the N50 mark in the read length histogram",
        action="store_true",
        default=False)
    visual.add_argument(
        "--title",
        help="Add a title to all plots, requires quoting if using spaces",
        type=str,
        default=None)

    target = parser.add_argument_group(
        title="Input data sources, one of these is required.")
    mtarget = target.add_mutually_exclusive_group(required=True)
    mtarget.add_argument(
        "--fastq",
        help="Data is in one or more default fastq file(s).",
        nargs='+',
        metavar="file")
    mtarget.add_argument(
        "--fasta",
        help="Data is in one or more fasta file(s).",
        nargs='+',
        metavar="file")
    mtarget.add_argument(
        "--fastq_rich",
        help=
        "Data is in one or more fastq file(s) generated by albacore or MinKNOW \
                             with additional information concerning channel and time.",
        nargs='+',
        metavar="file")
    mtarget.add_argument(
        "--fastq_minimal",
        help=
        "Data is in one or more fastq file(s) generated by albacore or MinKNOW \
                             with additional information concerning channel and time. \
                             Minimal data is extracted swiftly without elaborate checks.",
        nargs='+',
        metavar="file")
    mtarget.add_argument(
        "--summary",
        help="Data is in one or more summary file(s) generated by albacore.",
        nargs='+',
        metavar="file")
    mtarget.add_argument(
        "--bam",
        help="Data is in one or more sorted bam file(s).",
        nargs='+',
        metavar="file")
    mtarget.add_argument(
        "--cram",
        help="Data is in one or more sorted cram file(s).",
        nargs='+',
        metavar="file")
    mtarget.add_argument(
        "--pickle",
        help="Data is a pickle file stored earlier.",
        metavar="pickle")
    args = parser.parse_args()
    if args.no_N50:
        sys.stderr.write(
            'DeprecationWarning: --no-N50 is currently the default setting.\n')
        sys.stderr.write(
            'The argument is thus unnecessary but kept for backwards compatibility.'
        )
    return args


def main():
    args = get_args()
    settings = vars(args)
    settings["path"] = os.path.join(args.outdir, args.prefix)
    sources = {
        # "fastq": args.fastq,
        "bam": args.bam,
        # "cram": args.cram,
        # "fastq_rich": args.fastq_rich,
        # "fastq_minimal": args.fastq_minimal,
        # "summary": args.summary,
        # "fasta": args.fasta,
    }

    datadf = get_input(
        source=[n for n, s in sources.items() if s][0],
        files=[f for f in sources.values() if f][0],
        threads=args.threads,
        readtype=args.readtype,
        combine="simple",
        barcoded=args.barcoded)

    settings["lengths_pointer"] = "aligned_lengths" # only for bam
    # settings["length_prefix"] = 'length_prefix'
    datadf, settings = filter_and_transform_data(datadf, settings)
    plots = make_plots(datadf, settings)

if __name__ == '__main__':
    main()
